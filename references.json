[
  {
    "publisher": "Elsevier BV",
    "issue": "6",
    "DOI": "10.1016/j.tics.2005.04.010",
    "type": "article-journal",
    "page": "296-305",
    "source": "Crossref",
    "title": "Capacity limits of information processing in the brain",
    "volume": "9",
    "author": [
      {
        "given": "René",
        "family": "Marois"
      },
      {
        "given": "Jason",
        "family": "Ivanoff"
      }
    ],
    "container-title": "Trends in Cognitive Sciences",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2005,
          6
        ]
      ]
    },
    "URL": "https://doi.org/d5gmqt",
    "container-title-short": "Trends in Cognitive Sciences",
    "PMID": "15925809",
    "id": "q5vfV9nk",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.tics.2005.04.010"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7956",
    "DOI": "10.1038/s41586-023-05881-4",
    "type": "article-journal",
    "page": "259-265",
    "source": "Crossref",
    "title": "Foundation models for generalist medical artificial intelligence",
    "volume": "616",
    "author": [
      {
        "given": "Michael",
        "family": "Moor"
      },
      {
        "given": "Oishi",
        "family": "Banerjee"
      },
      {
        "given": "Zahra Shakeri Hossein",
        "family": "Abad"
      },
      {
        "given": "Harlan M.",
        "family": "Krumholz"
      },
      {
        "given": "Jure",
        "family": "Leskovec"
      },
      {
        "given": "Eric J.",
        "family": "Topol"
      },
      {
        "given": "Pranav",
        "family": "Rajpurkar"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          4,
          12
        ]
      ]
    },
    "URL": "https://doi.org/gr4td4",
    "container-title-short": "Nature",
    "PMID": "37045921",
    "id": "elx4isXx",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41586-023-05881-4"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "6",
    "DOI": "10.1038/s41587-023-01789-6",
    "type": "article-journal",
    "page": "750-751",
    "source": "Crossref",
    "title": "How will generative AI disrupt data science in drug discovery?",
    "volume": "41",
    "author": [
      {
        "given": "Jean-Philippe",
        "family": "Vert"
      }
    ],
    "container-title": "Nature Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          5,
          8
        ]
      ]
    },
    "URL": "https://doi.org/gsznzd",
    "container-title-short": "Nat Biotechnol",
    "PMID": "37156917",
    "id": "wo7jyZHW",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41587-023-01789-6"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "DOI": "10.1038/s41597-020-0486-7",
    "type": "article-journal",
    "source": "Crossref",
    "title": "The TRUST Principles for digital repositories",
    "volume": "7",
    "author": [
      {
        "given": "Dawei",
        "family": "Lin"
      },
      {
        "given": "Jonathan",
        "family": "Crabtree"
      },
      {
        "given": "Ingrid",
        "family": "Dillo"
      },
      {
        "given": "Robert R.",
        "family": "Downs"
      },
      {
        "given": "Rorie",
        "family": "Edmunds"
      },
      {
        "given": "David",
        "family": "Giaretta"
      },
      {
        "given": "Marisa",
        "family": "De Giusti"
      },
      {
        "given": "Hervé",
        "family": "L’Hours"
      },
      {
        "given": "Wim",
        "family": "Hugo"
      },
      {
        "given": "Reyna",
        "family": "Jenkyns"
      },
      {
        "given": "Varsha",
        "family": "Khodiyar"
      },
      {
        "given": "Maryann E.",
        "family": "Martone"
      },
      {
        "given": "Mustapha",
        "family": "Mokrane"
      },
      {
        "given": "Vivek",
        "family": "Navale"
      },
      {
        "given": "Jonathan",
        "family": "Petters"
      },
      {
        "given": "Barbara",
        "family": "Sierman"
      },
      {
        "given": "Dina V.",
        "family": "Sokolova"
      },
      {
        "given": "Martina",
        "family": "Stockhause"
      },
      {
        "given": "John",
        "family": "Westbrook"
      }
    ],
    "container-title": "Scientific Data",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          5,
          14
        ]
      ]
    },
    "URL": "https://doi.org/ggwrtj",
    "container-title-short": "Sci Data",
    "PMCID": "PMC7224370",
    "PMID": "32409645",
    "id": "MMp6cQ1x",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41597-020-0486-7"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:sec><jats:title>Motivation</jats:title><jats:p>Although gene set enrichment analysis has become an integral part of high-throughput gene expression data analysis, the assessment of enrichment methods remains rudimentary and ad hoc. In the absence of suitable gold standards, evaluations are commonly restricted to selected datasets and biological reasoning on the relevance of resulting enriched gene sets.</jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p>We develop an extensible framework for reproducible benchmarking of enrichment methods based on defined criteria for applicability, gene set prioritization and detection of relevant processes. This framework incorporates a curated compendium of 75 expression datasets investigating 42 human diseases. The compendium features microarray and RNA-seq measurements, and each dataset is associated with a precompiled GO/KEGG relevance ranking for the corresponding disease under investigation. We perform a comprehensive assessment of 10 major enrichment methods, identifying significant differences in runtime and applicability to RNA-seq data, fraction of enriched gene sets depending on the null hypothesis tested and recovery of the predefined relevance rankings. We make practical recommendations on how methods originally developed for microarray data can efficiently be applied to RNA-seq data, how to interpret results depending on the type of gene set test conducted and which methods are best suited to effectively prioritize gene sets with high phenotype relevance.</jats:p></jats:sec><jats:sec><jats:title>Availability</jats:title><jats:p>http://bioconductor.org/packages/GSEABenchmarkeR</jats:p></jats:sec><jats:sec><jats:title>Contact</jats:title><jats:p>ludwig.geistlinger@sph.cuny.edu</jats:p></jats:sec>",
    "DOI": "10.1093/bib/bbz158",
    "type": "article-journal",
    "page": "545-556",
    "source": "Crossref",
    "title": "Toward a gold standard for benchmarking gene set enrichment analysis",
    "volume": "22",
    "author": [
      {
        "given": "Ludwig",
        "family": "Geistlinger"
      },
      {
        "given": "Gergely",
        "family": "Csaba"
      },
      {
        "given": "Mara",
        "family": "Santarelli"
      },
      {
        "given": "Marcel",
        "family": "Ramos"
      },
      {
        "given": "Lucas",
        "family": "Schiffer"
      },
      {
        "given": "Nitesh",
        "family": "Turaga"
      },
      {
        "given": "Charity",
        "family": "Law"
      },
      {
        "given": "Sean",
        "family": "Davis"
      },
      {
        "given": "Vincent",
        "family": "Carey"
      },
      {
        "given": "Martin",
        "family": "Morgan"
      },
      {
        "given": "Ralf",
        "family": "Zimmer"
      },
      {
        "given": "Levi",
        "family": "Waldron"
      }
    ],
    "container-title": "Briefings in Bioinformatics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          3,
          9
        ]
      ]
    },
    "URL": "https://doi.org/ggs7tp",
    "PMCID": "PMC7820859",
    "PMID": "32026945",
    "id": "k6hpwdsR",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/bib/bbz158"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>ABSTRACT</jats:title><jats:p>Cell type annotation is an essential step in single-cell RNA-seq analysis. However, it is a time-consuming process that often requires expertise in collecting canonical marker genes and manually annotating cell types. Automated cell type annotation methods typically require the acquisition of high-quality reference datasets and the development of additional pipelines. We demonstrate that GPT-4, a highly potent large language model, can automatically and accurately annotate cell types by utilizing marker gene information generated from standard single-cell RNA-seq analysis pipelines. Evaluated across hundreds of tissue types and cell types, GPT-4 generates cell type annotations exhibiting strong concordance with manual annotations, and has the potential to considerably reduce the effort and expertise needed in cell type annotation.</jats:p>",
    "DOI": "10.1101/2023.04.16.537094",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Reference-free and cost-effective automated cell type annotation with GPT-4 in single-cell RNA-seq analysis",
    "author": [
      {
        "given": "Wenpin",
        "family": "Hou"
      },
      {
        "given": "Zhicheng",
        "family": "Ji"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          4,
          21
        ]
      ]
    },
    "URL": "https://doi.org/gsznzg",
    "PMCID": "PMC10153208",
    "PMID": "37131626",
    "id": "ae7XiPvs",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.04.16.537094"
  },
  {
    "type": "article",
    "id": "IzWFZmuQ",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Thoppilan",
        "given": "Romal"
      },
      {
        "family": "De Freitas",
        "given": "Daniel"
      },
      {
        "family": "Hall",
        "given": "Jamie"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Kulshreshtha",
        "given": "Apoorv"
      },
      {
        "family": "Cheng",
        "given": "Heng-Tze"
      },
      {
        "family": "Jin",
        "given": "Alicia"
      },
      {
        "family": "Bos",
        "given": "Taylor"
      },
      {
        "family": "Baker",
        "given": "Leslie"
      },
      {
        "family": "Du",
        "given": "Yu"
      },
      {
        "family": "Li",
        "given": "YaGuang"
      },
      {
        "family": "Lee",
        "given": "Hongrae"
      },
      {
        "family": "Zheng",
        "given": "Huaixiu Steven"
      },
      {
        "family": "Ghafouri",
        "given": "Amin"
      },
      {
        "family": "Menegali",
        "given": "Marcelo"
      },
      {
        "family": "Huang",
        "given": "Yanping"
      },
      {
        "family": "Krikun",
        "given": "Maxim"
      },
      {
        "family": "Lepikhin",
        "given": "Dmitry"
      },
      {
        "family": "Qin",
        "given": "James"
      },
      {
        "family": "Chen",
        "given": "Dehao"
      },
      {
        "family": "Xu",
        "given": "Yuanzhong"
      },
      {
        "family": "Chen",
        "given": "Zhifeng"
      },
      {
        "family": "Roberts",
        "given": "Adam"
      },
      {
        "family": "Bosma",
        "given": "Maarten"
      },
      {
        "family": "Zhao",
        "given": "Vincent"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Chang",
        "given": "Chung-Ching"
      },
      {
        "family": "Krivokon",
        "given": "Igor"
      },
      {
        "family": "Rusch",
        "given": "Will"
      },
      {
        "family": "Pickett",
        "given": "Marc"
      },
      {
        "family": "Srinivasan",
        "given": "Pranesh"
      },
      {
        "family": "Man",
        "given": "Laichee"
      },
      {
        "family": "Meier-Hellstern",
        "given": "Kathleen"
      },
      {
        "family": "Morris",
        "given": "Meredith Ringel"
      },
      {
        "family": "Doshi",
        "given": "Tulsee"
      },
      {
        "family": "Santos",
        "given": "Renelito Delos"
      },
      {
        "family": "Duke",
        "given": "Toju"
      },
      {
        "family": "Soraker",
        "given": "Johnny"
      },
      {
        "family": "Zevenbergen",
        "given": "Ben"
      },
      {
        "family": "Prabhakaran",
        "given": "Vinodkumar"
      },
      {
        "family": "Diaz",
        "given": "Mark"
      },
      {
        "family": "Hutchinson",
        "given": "Ben"
      },
      {
        "family": "Olson",
        "given": "Kristen"
      },
      {
        "family": "Molina",
        "given": "Alejandra"
      },
      {
        "family": "Hoffman-John",
        "given": "Erin"
      },
      {
        "family": "Lee",
        "given": "Josh"
      },
      {
        "family": "Aroyo",
        "given": "Lora"
      },
      {
        "family": "Rajakumar",
        "given": "Ravi"
      },
      {
        "family": "Butryna",
        "given": "Alena"
      },
      {
        "family": "Lamm",
        "given": "Matthew"
      },
      {
        "family": "Kuzmina",
        "given": "Viktoriya"
      },
      {
        "family": "Fenton",
        "given": "Joe"
      },
      {
        "family": "Cohen",
        "given": "Aaron"
      },
      {
        "family": "Bernstein",
        "given": "Rachel"
      },
      {
        "family": "Kurzweil",
        "given": "Ray"
      },
      {
        "family": "Aguera-Arcas",
        "given": "Blaise"
      },
      {
        "family": "Cui",
        "given": "Claire"
      },
      {
        "family": "Croak",
        "given": "Marian"
      },
      {
        "family": "Chi",
        "given": "Ed"
      },
      {
        "family": "Le",
        "given": "Quoc"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
    "DOI": "10.48550/arxiv.2201.08239",
    "publisher": "arXiv",
    "title": "LaMDA: Language Models for Dialog Applications",
    "URL": "https://doi.org/kmfc",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2201.08239"
  },
  {
    "type": "article",
    "id": "JIjeWPOb",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Chowdhery",
        "given": "Aakanksha"
      },
      {
        "family": "Narang",
        "given": "Sharan"
      },
      {
        "family": "Devlin",
        "given": "Jacob"
      },
      {
        "family": "Bosma",
        "given": "Maarten"
      },
      {
        "family": "Mishra",
        "given": "Gaurav"
      },
      {
        "family": "Roberts",
        "given": "Adam"
      },
      {
        "family": "Barham",
        "given": "Paul"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Sutton",
        "given": "Charles"
      },
      {
        "family": "Gehrmann",
        "given": "Sebastian"
      },
      {
        "family": "Schuh",
        "given": "Parker"
      },
      {
        "family": "Shi",
        "given": "Kensen"
      },
      {
        "family": "Tsvyashchenko",
        "given": "Sasha"
      },
      {
        "family": "Maynez",
        "given": "Joshua"
      },
      {
        "family": "Rao",
        "given": "Abhishek"
      },
      {
        "family": "Barnes",
        "given": "Parker"
      },
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Prabhakaran",
        "given": "Vinodkumar"
      },
      {
        "family": "Reif",
        "given": "Emily"
      },
      {
        "family": "Du",
        "given": "Nan"
      },
      {
        "family": "Hutchinson",
        "given": "Ben"
      },
      {
        "family": "Pope",
        "given": "Reiner"
      },
      {
        "family": "Bradbury",
        "given": "James"
      },
      {
        "family": "Austin",
        "given": "Jacob"
      },
      {
        "family": "Isard",
        "given": "Michael"
      },
      {
        "family": "Gur-Ari",
        "given": "Guy"
      },
      {
        "family": "Yin",
        "given": "Pengcheng"
      },
      {
        "family": "Duke",
        "given": "Toju"
      },
      {
        "family": "Levskaya",
        "given": "Anselm"
      },
      {
        "family": "Ghemawat",
        "given": "Sanjay"
      },
      {
        "family": "Dev",
        "given": "Sunipa"
      },
      {
        "family": "Michalewski",
        "given": "Henryk"
      },
      {
        "family": "Garcia",
        "given": "Xavier"
      },
      {
        "family": "Misra",
        "given": "Vedant"
      },
      {
        "family": "Robinson",
        "given": "Kevin"
      },
      {
        "family": "Fedus",
        "given": "Liam"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      },
      {
        "family": "Ippolito",
        "given": "Daphne"
      },
      {
        "family": "Luan",
        "given": "David"
      },
      {
        "family": "Lim",
        "given": "Hyeontaek"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      },
      {
        "family": "Spiridonov",
        "given": "Alexander"
      },
      {
        "family": "Sepassi",
        "given": "Ryan"
      },
      {
        "family": "Dohan",
        "given": "David"
      },
      {
        "family": "Agrawal",
        "given": "Shivani"
      },
      {
        "family": "Omernick",
        "given": "Mark"
      },
      {
        "family": "Dai",
        "given": "Andrew M."
      },
      {
        "family": "Pillai",
        "given": "Thanumalayan Sankaranarayana"
      },
      {
        "family": "Pellat",
        "given": "Marie"
      },
      {
        "family": "Lewkowycz",
        "given": "Aitor"
      },
      {
        "family": "Moreira",
        "given": "Erica"
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Polozov",
        "given": "Oleksandr"
      },
      {
        "family": "Lee",
        "given": "Katherine"
      },
      {
        "family": "Zhou",
        "given": "Zongwei"
      },
      {
        "family": "Wang",
        "given": "Xuezhi"
      },
      {
        "family": "Saeta",
        "given": "Brennan"
      },
      {
        "family": "Diaz",
        "given": "Mark"
      },
      {
        "family": "Firat",
        "given": "Orhan"
      },
      {
        "family": "Catasta",
        "given": "Michele"
      },
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Meier-Hellstern",
        "given": "Kathy"
      },
      {
        "family": "Eck",
        "given": "Douglas"
      },
      {
        "family": "Dean",
        "given": "Jeff"
      },
      {
        "family": "Petrov",
        "given": "Slav"
      },
      {
        "family": "Fiedel",
        "given": "Noah"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
    "DOI": "10.48550/arxiv.2204.02311",
    "publisher": "arXiv",
    "title": "PaLM: Scaling Language Modeling with Pathways",
    "URL": "https://doi.org/kfxf",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2204.02311"
  },
  {
    "type": "article",
    "id": "bzAtd1rg",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kojima",
        "given": "Takeshi"
      },
      {
        "family": "Gu",
        "given": "Shixiang Shane"
      },
      {
        "family": "Reid",
        "given": "Machel"
      },
      {
        "family": "Matsuo",
        "given": "Yutaka"
      },
      {
        "family": "Iwasawa",
        "given": "Yusuke"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
    "DOI": "10.48550/arxiv.2205.11916",
    "publisher": "arXiv",
    "title": "Large Language Models are Zero-Shot Reasoners",
    "URL": "https://doi.org/gr263v",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2205.11916"
  },
  {
    "type": "article",
    "id": "17lpGtuH5",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "literal": "OpenAI"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
    "DOI": "10.48550/arxiv.2303.08774",
    "publisher": "arXiv",
    "title": "GPT-4 Technical Report",
    "URL": "https://doi.org/grx4cb",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2303.08774"
  },
  {
    "type": "article",
    "id": "1BzQcjRCZ",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Shen",
        "given": "Yongliang"
      },
      {
        "family": "Song",
        "given": "Kaitao"
      },
      {
        "family": "Tan",
        "given": "Xu"
      },
      {
        "family": "Li",
        "given": "Dongsheng"
      },
      {
        "family": "Lu",
        "given": "Weiming"
      },
      {
        "family": "Zhuang",
        "given": "Yueting"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence.",
    "DOI": "10.48550/arxiv.2303.17580",
    "publisher": "arXiv",
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
    "URL": "https://doi.org/gskd97",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2303.17580"
  },
  {
    "id": "gy4YOpGJ",
    "type": "webpage",
    "abstract": "Stay up-to-date on the latest developments in artificial intelligence and natural language processing with the Official Auto-GPT Blog. Get insights into how GPT technology is transforming industries and changing the way we interact with machines. Subscribe today and join the conversation!",
    "container-title": "AutoGPT Official",
    "language": "en-US",
    "title": "AutoGPT Official",
    "URL": "https://autogpt.net/",
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          9,
          18
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://autogpt.net/"
  },
  {
    "id": "UEmjXz02",
    "type": "webpage",
    "language": "en",
    "title": "🦜️🔗 Langchain",
    "URL": "https://python.langchain.com/",
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          27
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://python.langchain.com"
  },
  {
    "id": "5mQXgjfA",
    "type": "article-newspaper",
    "abstract": "Scientists say focus should be on cancer prevention, with \"universal cures\" unlikely at present.",
    "container-title": "BBC News",
    "language": "en-GB",
    "section": "Health",
    "source": "www.bbc.com",
    "title": "Study reveals cancer’s ‘infinite’ ability to evolve",
    "URL": "https://www.bbc.com/news/health-65252510",
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          4,
          12
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://www.bbc.com/news/health-65252510"
  }
]
